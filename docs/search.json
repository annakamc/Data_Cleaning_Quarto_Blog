[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Intro Into the World of Data Cleaning",
    "section": "",
    "text": "If you‚Äôre curious about how real-world data becomes valuable insight, you‚Äôre in the right place.\nIn industry, raw data rarely arrives in a clean, ready-to-use format. It‚Äôs messy, incomplete, inconsistent, and often misleading. That‚Äôs where data cleaning steps in‚Äîa crucial but often underestimated part of the data science lifecycle.\nLet‚Äôs break it down a little:\n\n\nData engineers build the infrastructure that moves and stores data efficiently. This includes: - ETL (Extract, Transform, Load) pipelines - Cloud data lakes and warehouses - APIs and automation scripts\nWithout reliable engineering, data science wouldn‚Äôt even have usable data to work with.\n\n\n\nYou might hear people say, ‚Äú80% of data science is cleaning the data‚Äù‚Äîand they‚Äôre not wrong. Data cleaning involves: - Removing duplicates - Fixing missing or malformed values - Normalizing units and formats - Validating assumptions about the data\nIt‚Äôs like prepping your ingredients before cooking‚Äîyou can‚Äôt make a 5-star dish from rotten tomatoes üçÖ.\n\n\n\nData scientists model cleaned data to: - Forecast demand - Predict customer churn - Optimize business operations - Build recommendation engines\nBut none of this is possible without trust in the data.\n\n\n\nRegular expressions (regex) are patterns that help you search, extract, and transform data efficiently. It‚Äôs a Swiss Army knife for: - Feature engineering (pulling zip codes from addresses) - Data mining (finding hidden patterns) - Cleaning OCR outputs (like PDFs scanned through AWS Textract) - Standardizing formats (like phone numbers or codes)\nHere‚Äôs a fun Python regex example:\nimport re\n\ntext = \"Order Number: PO-12345 received on 2024-10-01\"\norder_id = re.search(r\"PO-\\d+\", text)\n\nif order_id:\n    print(f\"Extracted Order ID: {order_id.group()}\")\nThis pattern r‚ÄùPO-‚Äù tells Python to find a string that starts with ‚ÄúPO-‚Äù and is followed by one or more digits. That‚Äôs regex magic in action and don‚Äôt worry it gets a lot more advanced!\nWhether you‚Äôre debugging a Python script, writing SQL for dashboards, or exploring machine learning, regex and data cleaning skills will take you far. These are the foundations that make your work reproducible, scalable, and trustworthy.\nThis blog is here to walk through real examples, business use cases, and clean code patterns to help you become confident in every stage of the data workflow‚Äîfrom chaos to clarity.\nLet‚Äôs clean, wrangle, and regex our way to greatness!"
  },
  {
    "objectID": "posts/welcome/index.html#why-data-cleaning-and-regex-matter-in-data-science",
    "href": "posts/welcome/index.html#why-data-cleaning-and-regex-matter-in-data-science",
    "title": "Intro Into the World of Data Cleaning",
    "section": "",
    "text": "If you‚Äôre curious about how real-world data becomes valuable insight, you‚Äôre in the right place.\nIn industry, raw data rarely arrives in a clean, ready-to-use format. It‚Äôs messy, incomplete, inconsistent, and often misleading. That‚Äôs where data cleaning steps in‚Äîa crucial but often underestimated part of the data science lifecycle.\nLet‚Äôs break it down a little:\n\n\nData engineers build the infrastructure that moves and stores data efficiently. This includes: - ETL (Extract, Transform, Load) pipelines - Cloud data lakes and warehouses - APIs and automation scripts\nWithout reliable engineering, data science wouldn‚Äôt even have usable data to work with.\n\n\n\nYou might hear people say, ‚Äú80% of data science is cleaning the data‚Äù‚Äîand they‚Äôre not wrong. Data cleaning involves: - Removing duplicates - Fixing missing or malformed values - Normalizing units and formats - Validating assumptions about the data\nIt‚Äôs like prepping your ingredients before cooking‚Äîyou can‚Äôt make a 5-star dish from rotten tomatoes üçÖ.\n\n\n\nData scientists model cleaned data to: - Forecast demand - Predict customer churn - Optimize business operations - Build recommendation engines\nBut none of this is possible without trust in the data.\n\n\n\nRegular expressions (regex) are patterns that help you search, extract, and transform data efficiently. It‚Äôs a Swiss Army knife for: - Feature engineering (pulling zip codes from addresses) - Data mining (finding hidden patterns) - Cleaning OCR outputs (like PDFs scanned through AWS Textract) - Standardizing formats (like phone numbers or codes)\nHere‚Äôs a fun Python regex example:\nimport re\n\ntext = \"Order Number: PO-12345 received on 2024-10-01\"\norder_id = re.search(r\"PO-\\d+\", text)\n\nif order_id:\n    print(f\"Extracted Order ID: {order_id.group()}\")\nThis pattern r‚ÄùPO-‚Äù tells Python to find a string that starts with ‚ÄúPO-‚Äù and is followed by one or more digits. That‚Äôs regex magic in action and don‚Äôt worry it gets a lot more advanced!\nWhether you‚Äôre debugging a Python script, writing SQL for dashboards, or exploring machine learning, regex and data cleaning skills will take you far. These are the foundations that make your work reproducible, scalable, and trustworthy.\nThis blog is here to walk through real examples, business use cases, and clean code patterns to help you become confident in every stage of the data workflow‚Äîfrom chaos to clarity.\nLet‚Äôs clean, wrangle, and regex our way to greatness!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "Objective: Learn and apply regular expressions (Regex) in PySpark for feature engineering, data cleaning, and data transformation tasks.\n\n\n\nRegex (Regular Expressions) is a sequence of characters used to match patterns in text. It‚Äôs incredibly useful in data science and engineering workflows to: - Validate input - Extract values - Clean and format messy strings\nüëâ Recommended resources: - Apache Spark Regex Functions - Regexr Playground\n\n\n\n\nfrom pyspark.sql import SparkSession, functions as F\nspark = SparkSession.builder.getOrCreate()\n\nrlike() ‚Äì Filters rows matching a regex.\nregexp_extract() ‚Äì Extracts matching groups from text.\nregexp_replace() ‚Äì Replaces substrings matching regex.\n\n\n\n\n\n# Simple filter: find cadname containing '_s1' + 1-3 characters and ending in '.'\ndf = spark.table(\"pdda_pac.pac_silver_prod\")\ndf_filtered = df.filter(F.col(\"cadname\").rlike(r\"_s1.{1,3}\\.\")).limit(10)\ndf_filtered.display()\n\n\n\nemails = [(\"Alice\", \"alice@example.com\"), (\"Bob\", \"bob_at_example.com\")]\ndf_email = spark.createDataFrame(emails, [\"name\", \"email\"])\ndf_email = df_email.withColumn(\"domain\", F.regexp_extract(\"email\", r\"@([\\w.]+)\", 1))\ndf_email.display()\n\n\n\n\ninvoice_data = [(\"Invoice ID: 001 Date: 2023-03-15\",)]\ndf_invoice = spark.createDataFrame(invoice_data, [\"text\"])\ndf_invoice = df_invoice.withColumn(\"year\", F.regexp_extract(\"text\", r\"(?&lt;year&gt;\\d{4})\", 0))\ndf_invoice.display()\n\n\n\ndf_valid = df_email.filter(F.col(\"email\").rlike(r\"^\\w+@\\w+\\.\\w{2,3}$\"))\ndf_valid.display()\n\n\n\n\n\n\ndf_dates = df_invoice.withColumn(\n    \"date_only\",\n    F.regexp_extract(\"text\", r\"(?&lt;=Date: )\\d{4}-\\d{2}-\\d{2}\", 0)\n)\ndf_dates.display()\n\n\n\nlogs = [(\"User: John logged in at 14:23\",), (\"User: Jane logged out at 15:45\",)]\ndf_logs = spark.createDataFrame(logs, [\"log\"])\ndf_logs = df_logs.withColumn(\"in_time\", F.regexp_extract(\"log\", r\"^(?!.*out).*?(\\d{2}:\\d{2})\", 1))\ndf_logs.display()\n\n\n\n\n\nphones = [(\"(123) 456-7890\",), (\"987.654.3210\",)]\ndf_phones = spark.createDataFrame(phones, [\"phone\"])\ndf_phones = df_phones.withColumn(\"clean\", F.regexp_replace(\"phone\", r\"[^\\d]\", \"\"))\ndf_phones.display()\n\n\n\n\nRegex is more than just pattern matching‚Äîit‚Äôs a tool for transforming, cleaning, and engineering features from raw, messy text. Mastering it in PySpark makes you an incredibly effective data scientist or engineer.\nüî• Start small, test often, and remember: the best regex is the one that works and is readable!"
  },
  {
    "objectID": "posts/post-with-code/index.html#what-is-regex",
    "href": "posts/post-with-code/index.html#what-is-regex",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "Regex (Regular Expressions) is a sequence of characters used to match patterns in text. It‚Äôs incredibly useful in data science and engineering workflows to: - Validate input - Extract values - Clean and format messy strings\nüëâ Recommended resources: - Apache Spark Regex Functions - Regexr Playground"
  },
  {
    "objectID": "posts/post-with-code/index.html#key-pyspark-regex-functions",
    "href": "posts/post-with-code/index.html#key-pyspark-regex-functions",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "from pyspark.sql import SparkSession, functions as F\nspark = SparkSession.builder.getOrCreate()\n\nrlike() ‚Äì Filters rows matching a regex.\nregexp_extract() ‚Äì Extracts matching groups from text.\nregexp_replace() ‚Äì Replaces substrings matching regex."
  },
  {
    "objectID": "posts/post-with-code/index.html#beginner-pattern-matching-filtering",
    "href": "posts/post-with-code/index.html#beginner-pattern-matching-filtering",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "# Simple filter: find cadname containing '_s1' + 1-3 characters and ending in '.'\ndf = spark.table(\"pdda_pac.pac_silver_prod\")\ndf_filtered = df.filter(F.col(\"cadname\").rlike(r\"_s1.{1,3}\\.\")).limit(10)\ndf_filtered.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#basic-extraction",
    "href": "posts/post-with-code/index.html#basic-extraction",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "emails = [(\"Alice\", \"alice@example.com\"), (\"Bob\", \"bob_at_example.com\")]\ndf_email = spark.createDataFrame(emails, [\"name\", \"email\"])\ndf_email = df_email.withColumn(\"domain\", F.regexp_extract(\"email\", r\"@([\\w.]+)\", 1))\ndf_email.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#intermediate-grouping-boundaries",
    "href": "posts/post-with-code/index.html#intermediate-grouping-boundaries",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "invoice_data = [(\"Invoice ID: 001 Date: 2023-03-15\",)]\ndf_invoice = spark.createDataFrame(invoice_data, [\"text\"])\ndf_invoice = df_invoice.withColumn(\"year\", F.regexp_extract(\"text\", r\"(?&lt;year&gt;\\d{4})\", 0))\ndf_invoice.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#filtering-valid-emails",
    "href": "posts/post-with-code/index.html#filtering-valid-emails",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "df_valid = df_email.filter(F.col(\"email\").rlike(r\"^\\w+@\\w+\\.\\w{2,3}$\"))\ndf_valid.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#advanced-lookaheads-lookbehinds",
    "href": "posts/post-with-code/index.html#advanced-lookaheads-lookbehinds",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "df_dates = df_invoice.withColumn(\n    \"date_only\",\n    F.regexp_extract(\"text\", r\"(?&lt;=Date: )\\d{4}-\\d{2}-\\d{2}\", 0)\n)\ndf_dates.display()\n\n\n\nlogs = [(\"User: John logged in at 14:23\",), (\"User: Jane logged out at 15:45\",)]\ndf_logs = spark.createDataFrame(logs, [\"log\"])\ndf_logs = df_logs.withColumn(\"in_time\", F.regexp_extract(\"log\", r\"^(?!.*out).*?(\\d{2}:\\d{2})\", 1))\ndf_logs.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#real-world-cleaning-regex-replace",
    "href": "posts/post-with-code/index.html#real-world-cleaning-regex-replace",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "phones = [(\"(123) 456-7890\",), (\"987.654.3210\",)]\ndf_phones = spark.createDataFrame(phones, [\"phone\"])\ndf_phones = df_phones.withColumn(\"clean\", F.regexp_replace(\"phone\", r\"[^\\d]\", \"\"))\ndf_phones.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#summary",
    "href": "posts/post-with-code/index.html#summary",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "Regex is more than just pattern matching‚Äîit‚Äôs a tool for transforming, cleaning, and engineering features from raw, messy text. Mastering it in PySpark makes you an incredibly effective data scientist or engineer.\nüî• Start small, test often, and remember: the best regex is the one that works and is readable!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Intro:\nI‚Äôm Annaka‚Äîa technical data analyst at John Deere by day and a full-time student at BYU-I by night. Every day, I dive into complex data sets, transforming raw numbers into clear insights that drive strategic decision-making. Whether I‚Äôm using Databricks, GitHub, or AWS Textract, I absolutely love crafting regex to suit my needs and learning what more I can do with it.\nData isn‚Äôt just numbers to me; it‚Äôs like a puzzle waiting to be solved or a piece of art waiting to be understood. My passion lies in leveraging data to make meaningful impacts, constantly pushing the boundaries of what‚Äôs possible in data engineering, data science, and machine learning/artificial intelligence. This blog is my space to share the challenges, discoveries, and triumphs of cleaning and wrangling data into actionable stories.\nI invite you to join me on this journey. Let‚Äôs connect, collaborate, and geek out over everything from SQL and Python to Machine Learning, R, and more. If you‚Äôre passionate about data, love a good regex challenge, or just want to chat about the latest in ETL, data mining, and visualization techniques, feel free to reach out. Together, we can turn messy data into something truly extraordinary!\n### To give myself some credibility (aka why you should pay attention just a little)‚Ä¶\nI‚Äôve placed in four out of six hackathons, including one where my team‚Äôs machine learning code was actually implemented in production at Koch Industries‚Äîan experience that still blows my mind.\nI‚Äôm currently pursuing a Bachelor‚Äôs degree in Data Science with minors in Statistics and Business Analytics at BYU-Idaho. I will graduate in July with a 3.97 GPA while also having a job at John Deere as a technical data analyst working approximately 30 hours a week and also working on campus as the data lab tutor. Along the way, I‚Äôve earned certificates in Machine Learning and Databases from BYU-I, and completed the Google Data Analytics Professional Certificate‚Äîwhich unexpectedly launched me into some wild and wonderful opportunities.\nThat certificate led to me being featured in a Google YouTube episode, and eventually participating in AI beta testing with Google. I‚Äôve also done consulting work for over 5 companies, where I developed not just technical solutions, but also sharpened my communication, business vocabulary, KPI design, and creative problem-solving to meet each organization‚Äôs unique goals and resources.\nMy expertise in data and storytelling has opened doors for internal and external PR opportunities with Coursera, where I‚Äôve had articles written about my journey (linked on my LinkedIn page).\nI‚Äôm also currently leading a pilot program for BYU-Idaho‚Äîand potentially all BYU schools‚Äîto make Coursera freely available for students, aiming to bridge the gap between personalized learning and real-world, industry-ready experience.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home Page",
    "section": "",
    "text": "Regex Walkthrough\n\n\n\n\n\n\n\nData Cleaning\n\n\nRegex\n\n\nTips\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nAnnaka McClelland\n\n\n\n\n\n\n  \n\n\n\n\nReal-Life Examples of Data Cleaning with Regex\n\n\n\n\n\n\n\nReal-World\n\n\nCast Iron\n\n\nIEEE\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2025\n\n\nAnnaka McClelland\n\n\n\n\n\n\n  \n\n\n\n\nIntro Into the World of Data Cleaning\n\n\n\n\n\n\n\nData Cleaning\n\n\nIntro\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2025\n\n\nAnnaka McClelland\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/real-life-examples/index.html",
    "href": "posts/real-life-examples/index.html",
    "title": "Real-Life Examples of Data Cleaning with Regex",
    "section": "",
    "text": "Data cleaning is one of the most important‚Äîand often overlooked‚Äîsteps in any data workflow. In both industry and real life, clean data leads to clearer insights, better decisions, and stronger outcomes. Thanks for joining me on this journey into real-world examples and practical industry level tips!"
  },
  {
    "objectID": "posts/real-life-examples/index.html#project-background",
    "href": "posts/real-life-examples/index.html#project-background",
    "title": "Real-Life Examples of Data Cleaning with Regex",
    "section": "Project Background",
    "text": "Project Background\nI was tasked with extracting data from complex engineering PDF drawings using AWS Textract, and turning the output into a clean, relational dataframe. The goal? Meet the diverse needs of downstream teams‚Äîwhether for machine learning models, dashboards, or reporting insights‚Äîby making messy unstructured text clean and usable.\nTo do this, I built a modular data cleaning and transformation flow in Databricks using PySpark, powered by a growing suite of custom regex functions."
  },
  {
    "objectID": "posts/real-life-examples/index.html#regex-flow-overview",
    "href": "posts/real-life-examples/index.html#regex-flow-overview",
    "title": "Real-Life Examples of Data Cleaning with Regex",
    "section": "Regex Flow Overview",
    "text": "Regex Flow Overview\nThis is the end-to-end pipeline I designed, which this blog will break down (with a visual coming soon!):\ningest ‚Üí explode ‚Üí parse ‚Üí parse‚Ä¶n ‚Üí compile ‚Üí format ‚Üí productionize\n\nIngest: Read AWS Textract output from S3 into Databricks\nExplode: Flatten nested JSON text objects into rows\nParse: Extract key patterns or features with regex\nParse‚Ä¶n: Repeat parsing steps as needed, based on logic layers\nCompile: Append raw parsing outputs into a unified staging table\nFormat: Clean, reformat, and standardize columns for EDL ingestion\nProductionize: Load into EDL current for dashboards, ML, and analytics"
  },
  {
    "objectID": "posts/real-life-examples/index.html#the-real-star-parse...n",
    "href": "posts/real-life-examples/index.html#the-real-star-parse...n",
    "title": "Real-Life Examples of Data Cleaning with Regex",
    "section": "The Real Star: parse...n",
    "text": "The Real Star: parse...n\nThe core value of this project is in the regex parsing layers:\n\nüèóÔ∏è Enterprise standards are identified and extracted using flexible pattern matchers\nüßº Filters clean up misreads or irrelevant tokens\nüß† My favorite step: I built a PySpark function to clean, label, and enhance the extracted data\n\nAdds a description\nOutputs cleaned standard_text\nAdds contextual details like coating_type, layer_type, etc.\n\nüîÑ Then I used fx_feature_format to collapse the cleaned results into one clean output ready for the next step"
  },
  {
    "objectID": "posts/real-life-examples/index.html#shell-for-code-example",
    "href": "posts/real-life-examples/index.html#shell-for-code-example",
    "title": "Real-Life Examples of Data Cleaning with Regex",
    "section": "Shell for Code Example",
    "text": "Shell for Code Example\nHere‚Äôs where I‚Äôll walk you through a simplified version of the code. Stay tuned for deep dives!"
  }
]