[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Intro Into the World of Data Cleaning",
    "section": "",
    "text": "If you‚Äôre curious about how real-world data becomes valuable insight, you‚Äôre in the right place.\nIn industry, raw data rarely arrives in a clean, ready-to-use format. It‚Äôs messy, incomplete, inconsistent, and often misleading. That‚Äôs where data cleaning steps in‚Äîa crucial but often underestimated part of the data science lifecycle.\nLet‚Äôs break it down a little:\n\n\nData engineers build the infrastructure that moves and stores data efficiently. This includes: - ETL (Extract, Transform, Load) pipelines - Cloud data lakes and warehouses - APIs and automation scripts\nWithout reliable engineering, data science wouldn‚Äôt even have usable data to work with.\n\n\n\nYou might hear people say, ‚Äú80% of data science is cleaning the data‚Äù‚Äîand they‚Äôre not wrong. Data cleaning involves: - Removing duplicates - Fixing missing or malformed values - Normalizing units and formats - Validating assumptions about the data\nIt‚Äôs like prepping your ingredients before cooking‚Äîyou can‚Äôt make a 5-star dish from rotten tomatoes üçÖ.\n\n\n\nData scientists model cleaned data to: - Forecast demand - Predict customer churn - Optimize business operations - Build recommendation engines\nBut none of this is possible without trust in the data.\n\n\n\nRegular expressions (regex) are patterns that help you search, extract, and transform data efficiently. It‚Äôs a Swiss Army knife for: - Feature engineering (pulling zip codes from addresses) - Data mining (finding hidden patterns) - Cleaning OCR outputs (like PDFs scanned through AWS Textract) - Standardizing formats (like phone numbers or codes)\nHere‚Äôs a fun Python regex example:\nimport re\n\ntext = \"Order Number: PO-12345 received on 2024-10-01\"\norder_id = re.search(r\"PO-\\d+\", text)\n\nif order_id:\n    print(f\"Extracted Order ID: {order_id.group()}\")\nThis pattern r‚ÄùPO-‚Äù tells Python to find a string that starts with ‚ÄúPO-‚Äù and is followed by one or more digits. That‚Äôs regex magic in action and don‚Äôt worry it gets a lot more advanced!\nWhether you‚Äôre debugging a Python script, writing SQL for dashboards, or exploring machine learning, regex and data cleaning skills will take you far. These are the foundations that make your work reproducible, scalable, and trustworthy.\nThis blog is here to walk through real examples, business use cases, and clean code patterns to help you become confident in every stage of the data workflow‚Äîfrom chaos to clarity.\nLet‚Äôs clean, wrangle, and regex our way to greatness!"
  },
  {
    "objectID": "posts/welcome/index.html#why-data-cleaning-and-regex-matter-in-data-science",
    "href": "posts/welcome/index.html#why-data-cleaning-and-regex-matter-in-data-science",
    "title": "Intro Into the World of Data Cleaning",
    "section": "",
    "text": "If you‚Äôre curious about how real-world data becomes valuable insight, you‚Äôre in the right place.\nIn industry, raw data rarely arrives in a clean, ready-to-use format. It‚Äôs messy, incomplete, inconsistent, and often misleading. That‚Äôs where data cleaning steps in‚Äîa crucial but often underestimated part of the data science lifecycle.\nLet‚Äôs break it down a little:\n\n\nData engineers build the infrastructure that moves and stores data efficiently. This includes: - ETL (Extract, Transform, Load) pipelines - Cloud data lakes and warehouses - APIs and automation scripts\nWithout reliable engineering, data science wouldn‚Äôt even have usable data to work with.\n\n\n\nYou might hear people say, ‚Äú80% of data science is cleaning the data‚Äù‚Äîand they‚Äôre not wrong. Data cleaning involves: - Removing duplicates - Fixing missing or malformed values - Normalizing units and formats - Validating assumptions about the data\nIt‚Äôs like prepping your ingredients before cooking‚Äîyou can‚Äôt make a 5-star dish from rotten tomatoes üçÖ.\n\n\n\nData scientists model cleaned data to: - Forecast demand - Predict customer churn - Optimize business operations - Build recommendation engines\nBut none of this is possible without trust in the data.\n\n\n\nRegular expressions (regex) are patterns that help you search, extract, and transform data efficiently. It‚Äôs a Swiss Army knife for: - Feature engineering (pulling zip codes from addresses) - Data mining (finding hidden patterns) - Cleaning OCR outputs (like PDFs scanned through AWS Textract) - Standardizing formats (like phone numbers or codes)\nHere‚Äôs a fun Python regex example:\nimport re\n\ntext = \"Order Number: PO-12345 received on 2024-10-01\"\norder_id = re.search(r\"PO-\\d+\", text)\n\nif order_id:\n    print(f\"Extracted Order ID: {order_id.group()}\")\nThis pattern r‚ÄùPO-‚Äù tells Python to find a string that starts with ‚ÄúPO-‚Äù and is followed by one or more digits. That‚Äôs regex magic in action and don‚Äôt worry it gets a lot more advanced!\nWhether you‚Äôre debugging a Python script, writing SQL for dashboards, or exploring machine learning, regex and data cleaning skills will take you far. These are the foundations that make your work reproducible, scalable, and trustworthy.\nThis blog is here to walk through real examples, business use cases, and clean code patterns to help you become confident in every stage of the data workflow‚Äîfrom chaos to clarity.\nLet‚Äôs clean, wrangle, and regex our way to greatness!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "Objective: Learn and apply regular expressions (Regex) in PySpark for feature engineering, data cleaning, and data transformation tasks.\n\n\n\nRegex (Regular Expressions) is a sequence of characters used to match patterns in text. It‚Äôs incredibly useful in data science and engineering workflows to: - Validate input - Extract values - Clean and format messy strings\nüëâ Recommended resources: - Apache Spark Regex Functions - Regexr Playground\n\n\n\n\nfrom pyspark.sql import SparkSession, functions as F\nspark = SparkSession.builder.getOrCreate()\n\nrlike() ‚Äì Filters rows matching a regex.\nregexp_extract() ‚Äì Extracts matching groups from text.\nregexp_replace() ‚Äì Replaces substrings matching regex.\n\n\n\n\n\n# Simple filter: find cadname containing '_s1' + 1-3 characters and ending in '.'\ndf = spark.table(\"pdda_pac.pac_silver_prod\")\ndf_filtered = df.filter(F.col(\"cadname\").rlike(r\"_s1.{1,3}\\.\")).limit(10)\ndf_filtered.display()\n\n\n\nemails = [(\"Alice\", \"alice@example.com\"), (\"Bob\", \"bob_at_example.com\")]\ndf_email = spark.createDataFrame(emails, [\"name\", \"email\"])\ndf_email = df_email.withColumn(\"domain\", F.regexp_extract(\"email\", r\"@([\\w.]+)\", 1))\ndf_email.display()\n\n\n\n\ninvoice_data = [(\"Invoice ID: 001 Date: 2023-03-15\",)]\ndf_invoice = spark.createDataFrame(invoice_data, [\"text\"])\ndf_invoice = df_invoice.withColumn(\"year\", F.regexp_extract(\"text\", r\"(?&lt;year&gt;\\d{4})\", 0))\ndf_invoice.display()\n\n\n\ndf_valid = df_email.filter(F.col(\"email\").rlike(r\"^\\w+@\\w+\\.\\w{2,3}$\"))\ndf_valid.display()\n\n\n\n\n\n\ndf_dates = df_invoice.withColumn(\n    \"date_only\",\n    F.regexp_extract(\"text\", r\"(?&lt;=Date: )\\d{4}-\\d{2}-\\d{2}\", 0)\n)\ndf_dates.display()\n\n\n\nlogs = [(\"User: John logged in at 14:23\",), (\"User: Jane logged out at 15:45\",)]\ndf_logs = spark.createDataFrame(logs, [\"log\"])\ndf_logs = df_logs.withColumn(\"in_time\", F.regexp_extract(\"log\", r\"^(?!.*out).*?(\\d{2}:\\d{2})\", 1))\ndf_logs.display()\n\n\n\n\n\nphones = [(\"(123) 456-7890\",), (\"987.654.3210\",)]\ndf_phones = spark.createDataFrame(phones, [\"phone\"])\ndf_phones = df_phones.withColumn(\"clean\", F.regexp_replace(\"phone\", r\"[^\\d]\", \"\"))\ndf_phones.display()\n\n\n\n\nRegex is more than just pattern matching‚Äîit‚Äôs a tool for transforming, cleaning, and engineering features from raw, messy text. Mastering it in PySpark makes you an incredibly effective data scientist or engineer.\nüî• Start small, test often, and remember: the best regex is the one that works and is readable!"
  },
  {
    "objectID": "posts/post-with-code/index.html#what-is-regex",
    "href": "posts/post-with-code/index.html#what-is-regex",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "Regex (Regular Expressions) is a sequence of characters used to match patterns in text. It‚Äôs incredibly useful in data science and engineering workflows to: - Validate input - Extract values - Clean and format messy strings\nüëâ Recommended resources: - Apache Spark Regex Functions - Regexr Playground"
  },
  {
    "objectID": "posts/post-with-code/index.html#key-pyspark-regex-functions",
    "href": "posts/post-with-code/index.html#key-pyspark-regex-functions",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "from pyspark.sql import SparkSession, functions as F\nspark = SparkSession.builder.getOrCreate()\n\nrlike() ‚Äì Filters rows matching a regex.\nregexp_extract() ‚Äì Extracts matching groups from text.\nregexp_replace() ‚Äì Replaces substrings matching regex."
  },
  {
    "objectID": "posts/post-with-code/index.html#beginner-pattern-matching-filtering",
    "href": "posts/post-with-code/index.html#beginner-pattern-matching-filtering",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "# Simple filter: find cadname containing '_s1' + 1-3 characters and ending in '.'\ndf = spark.table(\"pdda_pac.pac_silver_prod\")\ndf_filtered = df.filter(F.col(\"cadname\").rlike(r\"_s1.{1,3}\\.\")).limit(10)\ndf_filtered.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#basic-extraction",
    "href": "posts/post-with-code/index.html#basic-extraction",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "emails = [(\"Alice\", \"alice@example.com\"), (\"Bob\", \"bob_at_example.com\")]\ndf_email = spark.createDataFrame(emails, [\"name\", \"email\"])\ndf_email = df_email.withColumn(\"domain\", F.regexp_extract(\"email\", r\"@([\\w.]+)\", 1))\ndf_email.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#intermediate-grouping-boundaries",
    "href": "posts/post-with-code/index.html#intermediate-grouping-boundaries",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "invoice_data = [(\"Invoice ID: 001 Date: 2023-03-15\",)]\ndf_invoice = spark.createDataFrame(invoice_data, [\"text\"])\ndf_invoice = df_invoice.withColumn(\"year\", F.regexp_extract(\"text\", r\"(?&lt;year&gt;\\d{4})\", 0))\ndf_invoice.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#filtering-valid-emails",
    "href": "posts/post-with-code/index.html#filtering-valid-emails",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "df_valid = df_email.filter(F.col(\"email\").rlike(r\"^\\w+@\\w+\\.\\w{2,3}$\"))\ndf_valid.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#advanced-lookaheads-lookbehinds",
    "href": "posts/post-with-code/index.html#advanced-lookaheads-lookbehinds",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "df_dates = df_invoice.withColumn(\n    \"date_only\",\n    F.regexp_extract(\"text\", r\"(?&lt;=Date: )\\d{4}-\\d{2}-\\d{2}\", 0)\n)\ndf_dates.display()\n\n\n\nlogs = [(\"User: John logged in at 14:23\",), (\"User: Jane logged out at 15:45\",)]\ndf_logs = spark.createDataFrame(logs, [\"log\"])\ndf_logs = df_logs.withColumn(\"in_time\", F.regexp_extract(\"log\", r\"^(?!.*out).*?(\\d{2}:\\d{2})\", 1))\ndf_logs.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#real-world-cleaning-regex-replace",
    "href": "posts/post-with-code/index.html#real-world-cleaning-regex-replace",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "phones = [(\"(123) 456-7890\",), (\"987.654.3210\",)]\ndf_phones = spark.createDataFrame(phones, [\"phone\"])\ndf_phones = df_phones.withColumn(\"clean\", F.regexp_replace(\"phone\", r\"[^\\d]\", \"\"))\ndf_phones.display()"
  },
  {
    "objectID": "posts/post-with-code/index.html#summary",
    "href": "posts/post-with-code/index.html#summary",
    "title": "Regex Walkthrough",
    "section": "",
    "text": "Regex is more than just pattern matching‚Äîit‚Äôs a tool for transforming, cleaning, and engineering features from raw, messy text. Mastering it in PySpark makes you an incredibly effective data scientist or engineer.\nüî• Start small, test often, and remember: the best regex is the one that works and is readable!"
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "About Me",
    "section": "",
    "text": "Intro:\nI‚Äôm Annaka - a data engineer at John Deere. Every day, I dive into complex data sets, transforming raw numbers into clear insights that drive strategic decision-making. Whether I‚Äôm using Databricks, GitHub, or AWS Textract, I absolutely love crafting regex to suit my needs and learning what more I can do with it.\nData isn‚Äôt just numbers to me; it‚Äôs like a puzzle waiting to be solved or a piece of art waiting to be understood. My passion lies in leveraging data to make meaningful impacts, constantly pushing the boundaries of what‚Äôs possible in data engineering, data science, and machine learning/artificial intelligence. This blog is my space to share the challenges, discoveries, and triumphs of cleaning and wrangling data into actionable stories.\nI invite you to join me on this journey. Let‚Äôs connect, collaborate, and geek out over everything from SQL and Python to Machine Learning, R, and more. If you‚Äôre passionate about data, love a good regex challenge, or just want to chat about the latest in ETL, data mining, and visualization techniques, feel free to reach out. Together, we can turn messy data into something truly extraordinary!\n\n\nTo give some credibility (aka why you should pay attention just a little)‚Ä¶\nI‚Äôve placed in four out of six hackathons, including one where my team‚Äôs machine learning code was actually implemented in production at Koch Industries‚Äîan experience that still blows my mind.\nI‚Äôm attained a Bachelor‚Äôs degree in Data Science with minors in Statistics and Business Analytics at BYU-Idaho. I graduated in 2025 with a 3.97 GPA while also having a job at John Deere as a technical data analyst, working approximately 30 hours a week, and also working on campus as the data lab tutor. Along the way, I‚Äôve earned certificates in Machine Learning and Databases from BYU-I, and completed the Google Data Analytics Professional Certificate‚Äîwhich unexpectedly launched me into some wild and wonderful opportunities.\nThat certificate led to me being featured in a Google YouTube episode, and eventually participating in AI beta testing with Google. I‚Äôve also done consulting work for over 5 companies, where I developed not just technical solutions, but also sharpened my communication, business vocabulary, KPI design, and creative problem-solving to meet each organization‚Äôs unique goals and resources.\nMy expertise in data and storytelling has opened doors for internal and external PR opportunities with Coursera, where I‚Äôve had articles written about my journey (linked on my LinkedIn page).\nI‚Äôm also currently leading a pilot program for BYU-Idaho‚Äîand potentially all BYU schools‚Äîto make Coursera freely available for students, aiming to bridge the gap between personalized learning and real-world, industry-ready experience.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home Page",
    "section": "",
    "text": "Regex Walkthrough\n\n\n\n\n\n\n\nData Cleaning\n\n\nRegex\n\n\nTips\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nAnnaka McClelland\n\n\n\n\n\n\n  \n\n\n\n\nReal-Life Examples of Data Cleaning with Regex\n\n\n\n\n\n\n\nReal-World\n\n\nRegex\n\n\nData Cleaning\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2025\n\n\nAnnaka McClelland\n\n\n\n\n\n\n  \n\n\n\n\nIntro Into the World of Data Cleaning\n\n\n\n\n\n\n\nData Cleaning\n\n\nIntro\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2025\n\n\nAnnaka McClelland\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/real-life-examples/index.html",
    "href": "posts/real-life-examples/index.html",
    "title": "Real-Life Examples of Data Cleaning with Regex",
    "section": "",
    "text": "Data cleaning is one of the most important‚Äîand often overlooked‚Äîsteps in any data workflow. In both industry and real life, clean data leads to clearer insights, better decisions, and stronger outcomes. Thanks for joining me on this journey into real-world examples and practical industry level tips!"
  },
  {
    "objectID": "posts/real-life-examples/index.html#project-background",
    "href": "posts/real-life-examples/index.html#project-background",
    "title": "Real-Life Examples of Data Cleaning with Regex",
    "section": "Project Background",
    "text": "Project Background\nI was tasked with extracting data from complex engineering PDF drawings using AWS Textract, and turning the output into a clean, relational dataframe. The goal? Meet the diverse needs of downstream teams‚Äîwhether for machine learning models, dashboards, or reporting insights‚Äîby making messy unstructured text clean and usable.\nTo do this, I built a modular data cleaning and transformation flow in Databricks using PySpark, powered by a growing suite of custom regex functions."
  },
  {
    "objectID": "posts/real-life-examples/index.html#regex-flow-overview",
    "href": "posts/real-life-examples/index.html#regex-flow-overview",
    "title": "Real-Life Examples of Data Cleaning with Regex",
    "section": "Regex Flow Overview",
    "text": "Regex Flow Overview\nThis is the end-to-end pipeline I designed, which this blog will break down (with a visual coming soon!):\ningest ‚Üí explode ‚Üí parse ‚Üí parse‚Ä¶n ‚Üí compile ‚Üí format ‚Üí productionize\n\nIngest: Read AWS Textract output from S3 into Databricks\nExplode: Flatten nested JSON text objects into rows\nParse: Extract key patterns or features with regex\nParse‚Ä¶n: Repeat parsing steps as needed, based on logic layers\nCompile: Append raw parsing outputs into a unified staging table\nFormat: Clean, reformat, and standardize columns for EDL ingestion\nProductionize: Load into EDL current for dashboards, ML, and analytics"
  },
  {
    "objectID": "posts/real-life-examples/index.html#the-real-star-parse...n",
    "href": "posts/real-life-examples/index.html#the-real-star-parse...n",
    "title": "Real-Life Examples of Data Cleaning with Regex",
    "section": "The Real Star: parse...n",
    "text": "The Real Star: parse...n\nThe core value of this project is in the regex parsing layers:\n\nüèóÔ∏è Enterprise standards are identified and extracted using flexible pattern matchers\nüßº Filters clean up misreads or irrelevant tokens\nüß† My favorite step: I built a PySpark function to clean, label, and enhance the extracted data\n\nAdds a description\nOutputs cleaned standard_text\nAdds contextual details like coating_type, layer_type, etc.\n\nüîÑ Then I used fx_feature_format to collapse the cleaned results into one clean output ready for the next step"
  },
  {
    "objectID": "posts/real-life-examples/index.html#code-example",
    "href": "posts/real-life-examples/index.html#code-example",
    "title": "Real-Life Examples of Data Cleaning with Regex",
    "section": "Code Example",
    "text": "Code Example\n\nFunction Purpose\nThe fx_find_paint_standards function is designed to clean and extract valuable engineering metadata‚Äîspecifically paint callouts‚Äîfrom AWS Textract output. These paint codes are often misread and inconsistently formatted, so this PySpark function applies rigorous pattern matching using regular expressions to pull out standardized insights.\n\n\n\nStep-by-Step Breakdown\n\n\nExample Data\nexample_list = [\"zz\", \"x3\", \"ki\"]\nexample_dictionary = {\n  \"zz\": \"Black primer\",\n  \"x3\": \"Durability level 3\",\n  \"ki\": \"Blue gloss coat\"\n}\ncolor_mapping_expr = F.create_map([F.lit(x) for x in itertools.chain(*color_mapping.items())])\nPurpose: Creates a Spark MapType expression from a Python dictionary (color_mapping) so it can be used to map color codes to color names.\nrow_count = df.count()\nPurpose: Ensures the function only runs if there are rows in the DataFrame.\n\n\n\n\nStep: Topcoat Color Extraction\n.withColumn('topcoat_color_code',\n  F.when(F.col('text_lower').rlike(r'[ji1][a-z]{2}f[\\d|i]\\dkii'), ...)\nThis block determines what the topcoat color code is based on:\n\nrlike(r'[ji1][a-z]{2}f[\\d|i]\\dkii'): Regex searches for misread variants of the enterprise standard.\nsubstring(..., loc + std_len, 2): Grabs 2 characters after the standard to find color code.\n\n\nRegex Breakdown:\n\njdmf[\\d|i]\\dkii: Matches strings like jdmf14kii or jdmfi4kii, accounting for misreads like i instead of 1.\nsubstring(..., 2): Extracts potential two-character color code like zz.\n\n\n\n\n\nStep: Temperature Classification\n.withColumn('temp_class',\n  F.when(F.col('topcoat_color_code').isNull(), None)\n  .when(F.trim(F.expr(\"substring(... + topcoat_color_code_len, 1)\"))\nThis block identifies the temperature class:\n\nChecks for common misreads like (, l, /, \\ interpreted as 1\nExtracts the next character after topcoat color to assign class\n\n\n\n\nStep: Physical Property Classification\n.withColumn('physical_property_class',\n  F.when(F.col('temp_class').isNull(), None)\nIf the temperature class is defined, we then extract and classify the physical property using a or b as per JDMF14 standard.\n\nDescription Mapping:\n\na: High-visibility parts (e.g., visible under normal operation)\nb: Low-visibility or non-critical parts\n\n\n\n\n\nStep: Primer Color Extraction\n.withColumn('primer_color_code',\n  F.when(substring(...).isin('77', 'z7', '7z'), 'zz')\nPrimer codes are also often misread and need correcting:\n\nFixes 7z, z7 to zz\nPulls primer color from after the topcoat color + temperature + physical property code\n\n\nRegex Highlight:\n\njdmf14[a-z]{1,2}[0-4][a|b][\\+|\\-|\\=]. matches valid JDMF14 callouts with a primer code\n\n\n\n\n\nStep: Additional Requirements\n.withColumn('additional_requirements',\n  F.when(...).otherwise(F.trim(F.col('additional_requirements_prep')))\nThis part:\n\nCaptures trailing codes like x3, x5, x1\nHandles common OCR misreads: xl = x1, xt = x7, xo = x0\nCleans up noisy strings that don‚Äôt conform to expected patterns\n\n\nRegex Breakdown:\n\n(?&lt;!x)([0-7])|[^x|x0|x1|x2|x3|x4|x5|x6|x7]: Matches invalid characters outside the x0‚Äìx7 pattern group\n[(|c](.*?)\\): Captures data inside parentheses like (x3,x5)\n\n\n\n\n\nAdditional Highlights\n\nFixing Color Misreads:\n.withColumn('topcoat_color_desc', color_mapping_expr[F.col('topcoat_color_code')])\nLinks each color code like zz, ki, h2 to readable names like ‚ÄúBlack primer‚Äù or ‚ÄúBlue gloss coat.‚Äù\n\n\nFinal Formatting and Output:\n.select(...).withColumns({...})\nEnsures: - All fields are either null or cleaned - Values are deduplicated by CAD part - Output includes page number and coordinates for tracing source data\n\n\n\n\nSummary\nThis function turns messy engineering paint callouts into clean, structured features ready for use in:\n\nMachine learning\nOperational dashboards\nStandards tracking\n\nRegex is the workhorse behind the curtain‚Äîhandling OCR misreads, non-standard delimiters, and cascading dependencies.\nLet me know if you want this packaged for a GitHub wiki or auto-generated with docs!"
  }
]